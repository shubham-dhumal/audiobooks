{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 : Creating a Model for AudioBook App using Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Create a Model using Neural Networks that can predict if a customer will buy again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The data is related to the audio book app which consists of purchasing behaviour of the customer . Each customer in the data set has made purchase atleast once. Hence we will create a model  based on the data that will predict if the customer will buy again from the audio book company. The main idea is that company should not spend its advertising budget on customers who are unlikely to purchase again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data info\n",
    "##### The columns names are not included in the dataset as we want no text in our data while training the model.\n",
    "##### Each row represents a person.\n",
    "##### 1st column - Customer ID,\n",
    "##### 2nd column - Overall Book Length (Its the sum of the length of all purchases)(mins),\n",
    "##### 3rd column - Average Book length (Its is the sum divided by number of purchases)(mins),\n",
    "##### 4th column - Overall price paid,\n",
    "##### 5th column - Average price paid,\n",
    "##### 6th column - Review ( 1 = left a review , 0 = din't left a review),\n",
    "##### 7th column - Review 10/10 ( Its measures review of a customer from 1 to 10),\n",
    "##### 8th column - Total minutes Listened,\n",
    "##### 9th column - Completion (It is the total minutes listened / Overall Book Length),\n",
    "##### 10th column - Support Requests ( total number of support requests made ),\n",
    "##### 11th column - Last visited minus Purchase date,\n",
    "##### 12th column - Targets (1 = If the customer bought again , 0 = If the customer din't buy )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT RELEVANT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_csv_data = np.loadtxt('D:\\original.csv',delimiter=',')\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALANCING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter +=1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all,indices_to_remove,axis=0)\n",
    "targets_equals_priors = np.delete(targets_all,indices_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARDIZE THE INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHUFFLE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equals_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT DATA INTO TRAIN, VALIDATION & TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the three datasets in .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train',inputs=train_inputs, targets= train_targets)\n",
    "np.savez('Audiobooks_data_validation',inputs=validation_inputs, targets= validation_targets)\n",
    "np.savez('Audiobooks_data_test',inputs=test_inputs, targets= test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float),npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float),npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE, OPTIMIZERS, LOSS, EARLY STOPPING & TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 1s - loss: 0.5892 - accuracy: 0.6672 - val_loss: 0.5071 - val_accuracy: 0.7315\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.4585 - accuracy: 0.7639 - val_loss: 0.4427 - val_accuracy: 0.7651\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.4097 - accuracy: 0.7877 - val_loss: 0.4090 - val_accuracy: 0.7942\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3847 - accuracy: 0.7949 - val_loss: 0.3946 - val_accuracy: 0.7987\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.3690 - accuracy: 0.8058 - val_loss: 0.3797 - val_accuracy: 0.8143\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.3581 - accuracy: 0.8139 - val_loss: 0.3720 - val_accuracy: 0.8166\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.3516 - accuracy: 0.8072 - val_loss: 0.3726 - val_accuracy: 0.8233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17971980888>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                              tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                              tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                              tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping()\n",
    "\n",
    "model.fit(train_inputs,\n",
    "          train_targets,\n",
    "          batch_size = batch_size,\n",
    "          epochs = max_epochs,\n",
    "          callbacks =[early_stopping],\n",
    "          validation_data =(validation_inputs,validation_targets),\n",
    "          verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 0s 300us/sample - loss: 0.3448 - accuracy: 0.8058\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
